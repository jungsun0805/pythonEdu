{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타(이미지 분석 gradio 예제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pythonEdu\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import gradio as gr\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_img(img_path):\n",
    "    \"\"\"Encode image to base64 string\"\"\"\n",
    "    with open(img_path, 'rb') as img_file:  # rb: 바이너리 읽기(이미지는 0,1 형태의 바이너리)\n",
    "        return base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_with_gemma(img):\n",
    "    \"\"\"Ollama Gemma3 4B model을 사용하여 이미지 분석\"\"\"\n",
    "    try:\n",
    "        # Convert Gradio image to PIL Image\n",
    "        pil_img = Image.fromarray(img)    # PIL(이미지처리, 조작) 이미지 객체로 변환\n",
    "        \n",
    "        # Save image to a temporary buffer\n",
    "        buff = io.BytesIO()    # 메모리 상에 임시 버퍼를 만들어서 이미지 데이터 저장\n",
    "        pil_img.save(buff,format='PNG')\n",
    "        \n",
    "        # Encode image to base64\n",
    "        encode_img = base64.b64encode(buff.getvalue()).decode('utf-8')    # 임시 버퍼에 저장된 이미지 데이터를 바이트 문자열로 가져옴\n",
    "        \n",
    "        # Perform image analysis using Ollama Gemma3\n",
    "        response = ollama.chat(\n",
    "            model = 'gemma3:4b',\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': '이미지의 내용을 자세히 설명해줘. 무엇이 보이는지, 색상, 구성, 감정 등을 포함해서 분석해줘.',\n",
    "                    'images': [encode_img]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"이미지 분석 중 오류 발생: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    fn = analyze_image_with_gemma,\n",
    "    inputs = gr.Image(type='numpy', label=\"이미지 업로드\"),\n",
    "    outputs = gr.Textbox(label=\"이미지 분석결과\"),\n",
    "    title = \"Ollama Gemma3 이미지 분석기\",\n",
    "    description=\"이미지를 업로드하면 Gemma3 4B 모델이 분석해드립니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset2.csv\n"
     ]
    }
   ],
   "source": [
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(역할 부여)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install ollama\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇의 기본적 질문, 답변 역할\n",
    "def ask_ollama(question):\n",
    "    # ollama를 사용하여 모델로부터 응답 생성\n",
    "    cr = \"너는 의사야. 질문에 대한 답은 3줄 이내로 짧게해줘.\"\n",
    "    res = ollama.chat(model=\"gemma2\", messages=[\n",
    "        {'role':'system', 'content': cr},    # 챗봇의 기본 역할 부여\n",
    "        {'role':'user', 'content': question}        # 질문\n",
    "    ])\n",
    "    \n",
    "    return res['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발목 통증은 부상으로 인한 것이 가능합니다.  \n",
      "\n",
      "휴식, 얼음찜질, 가벼운 압박을 해주세요. 만약 통증이 심하거나 움직이기 어렵다면 병원에 방문하세요.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"3일전에 축구한 이후로 발목이 아파\"\n",
    "res = ask_ollama(q)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(Gradio 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install gradio\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage    # HumanMessage: 사용자가 보낸 메시지, AIMessage : LLM의 메시지\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "model = ChatOllama(model=\"gemma3:4b\", temperature=0.1, verbose=False)    # temperture가 낮을수록 거의 동일답변, 높을수록 창의적인 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 기록을 포함하여 응답을 생성하는 함수\n",
    "def chat(msg, his):\n",
    "    # 이전 대화 기록을 ChatOllama 형식으로 변환\n",
    "    chat_his = []\n",
    "    for human, ai in his:\n",
    "        chat_his.append(HumanMessage(content=human))\n",
    "        chat_his.append(AIMessage(content=ai))\n",
    "        \n",
    "    # 현재 메시지 추가\n",
    "    chat_his.append(HumanMessage(content=msg))\n",
    "    \n",
    "    # 모델을 사용하여 응답 생성\n",
    "    res = model.invoke(chat_his)\n",
    "    \n",
    "    return res.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pythonEdu\\.venv\\lib\\site-packages\\gradio\\chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn = chat,\n",
    "    examples=[\n",
    "        \"안녕하세요!\",\n",
    "        \"인공지능에 대해 설명해주세요.\",\n",
    "        \"파이썬의 장점은 무엇인가요?\"\n",
    "    ],\n",
    "    title='AI 챗봇',\n",
    "    description=\"질문을 입력하면 AI가 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(Gradio + csv 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage   # HumanMessage: 사용자가 보낸 메시지, AIMessage : LLM의 메시지\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter  # 특정 문자(예: 줄바꿈, 공백)를 기준으로 텍스트를 분할하는 기능을 제공\n",
    "from langchain.chains import ConversationalRetrievalChain  # 질문과 관련된 정보 검색, 이전 대화정보도 같이 LLM에 제공, 답변생성\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"dataset/indata_kor.csv\", encoding='CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "txt_split = CharacterTextSplitter(chunk_size=1000,chunk_overlap=200)    # 텍스트 조각 최대 1000자, 텍스트 조각 사이에 200자만큼의 중복을 허용(문맥 유지)\n",
    "txts = txt_split.split_text('\\n'.join(df.to_string()))    # 문자열들을 줄바꿈 문자(\\n)를 기준으로 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델 초기화\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "# 모델 이름 : 조직이름(sentence-transformers) 다양한 작업 가능(all)-MS사 경령화 트랜스포머모델(MiniLM)-모델의 레이어수(L6)-모델이 버전(v2)\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 데이터베이스 생성\n",
    "vector = FAISS.from_texts(txts, emb)    # from_texts : 임베딩으로 변환된 벡터를 FAISS 인덱스에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "llm = ChatOllama(model=\"gemma3:4b\", temperature=0.1)    # temperture가 낮을수록 거의 동일답변, 높을수록 창의적인 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vector.as_retriever(search_kwargs={\"k\":1}),  # 가장 관련성 높은 1개의 문서만 검색\n",
    "    return_source_documents=True,    # 참고한 소스 문서 정보 반환 여부\n",
    "    verbose=False    # 체인의 실행 과정 출력 여부\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 함수 정의\n",
    "def chat(msg,his):\n",
    "    # 이전 대화 기록을 ConversationalRetrievalChain 형식으로 변환\n",
    "    chat_his = [(human,ai) for human, ai in his]\n",
    "    \n",
    "    # 모델을 사용하여 응답 생성\n",
    "    res = qa_chain({\"question\": msg, \"chat_history\":chat_his})\n",
    "    \n",
    "    # 소스 문서 정보 추출\n",
    "    sources = set([doc.metadata.get('source','Unknown') for doc in res['source_documents']])\n",
    "    source_info = f\"\\n\\n참고 출처: {', '.join(sources)}\" if sources else \"\"\n",
    "    \n",
    "    return res['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\pythonEdu\\.venv\\lib\\site-packages\\gradio\\chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn = chat,\n",
    "    examples=[\n",
    "        \"한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?\",\n",
    "        \"스마트금융과에 대해 설명해주세요\",\n",
    "        \"한국폴리텍대한 추천할만한 학과 하나를 소개해주세요.\"\n",
    "    ],\n",
    "    title = \"대학 정보 AI 챗봇\",\n",
    "    description=\"스마크금융과에 대한 질문을 입력하면 AI가 CSV데이터를 참고하여 한글로 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human-16\\AppData\\Local\\Temp\\ipykernel_3612\\2545116048.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = qa_chain({\"question\": msg, \"chat_history\":chat_his})\n"
     ]
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(인터넷 URL정보 요약하기 + TAB추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface\n",
    "\n",
    "# iface = gr.Interface(\n",
    "#     fn=rag_chain,\n",
    "#     inputs=[\"text\", \"text\"],\n",
    "#     outputs=\"text\",\n",
    "#     title=\"RAG Chain Question Answering\",\n",
    "#     description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Tabbed Interface\n",
    "    # Tab for Question and Answer\n",
    "    # Tab for Visualization (Word Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(STT:음성을 텍스트로 전환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install openai-whisper\n",
    "# ffmpeg 사전 설치 및 환경변수 path 설정(경로/bin)\n",
    "import os\n",
    "from dotenv import load_dotenv  # 환경변수 로드가 필요한 경우\n",
    "import whisper\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .env 파일에서 환경 변수 로드 (필요한 경우)\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg 경로 명시적 설정\n",
    "# os.environ[\"FFMPEG_BINARY\"] = \"C:/aiproject/ffmpeg/bin/ffmpeg.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\ffmpeg\\bin\"\n",
    "os.environ[\"FFMPEG_BINARY\"] = r\"C:\\ffmpeg\\bin\\ffmpeg.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Whisper 모델 로드\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # 오디오 파일 전사\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # 전사된 텍스트 반환\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"오디오 파일을 업로드해주세요.\"\n",
    "    try:\n",
    "        transcribed_text = transcribe_audio(audio)\n",
    "        return transcribed_text\n",
    "    except Exception as e:\n",
    "        return f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"MP3 파일 업로드\"),\n",
    "    outputs=\"text\",\n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description=\"MP3 파일을 업로드하면 텍스트로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:13<00:00, 11.1MiB/s]\n",
      "c:\\pythonEdu\\.venv\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(TTS:텍스트를 음성변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install gtts\n",
    "import gradio as gr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_spch(txt, lang='ko')    :\n",
    "    # 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False,suffix=\".mp3\") as fp:\n",
    "        temp_filename = fp.name\n",
    "    # TTS 변환\n",
    "    tts = gTTS(text=txt,lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(txt, lang):\n",
    "    if not txt:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = txt_to_spch(txt, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생또는 다운로드할 수있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류발생: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradion 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn = process_tts,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=5, label=\"텍스트 입력\"),\n",
    "        gr.Dropdown(choices=['ko','en','ja','zh-cn'], label='언어 선택', value='ko')\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"생성된 오디오\"),\n",
    "        gr.Textbox(label=\"상태 메시지\")\n",
    "    ],\n",
    "    title=\"Text to Speech Converter\",\n",
    "    description=\"텍스트를 입력하면 MP3 파일로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 분류 예제(Gradio 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전설치 : pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow MobileNetV2 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # URL에서 이미지 가져오기\n",
    "            # BytesIO 사용하여 이미지 열기\n",
    "        # 이미지를 배열로 변환\n",
    "            # 배치 차원 추가\n",
    "            # 전처리\n",
    "        # 예측 수행\n",
    "            # 상위 3개 예측 결과 반환\n",
    "        # Gradio Label 컴포넌트에 맞게 결과 형식 변경\n",
    "        # {label1: confidence1, label2: confidence2, ...} 형식으로 반환\n",
    "            # 에러 발생 시 기본값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터페이스 실행\n",
    "# 예시 이미지 URL : https://health.chosun.com/site/data/img_dir/2024/04/19/2024041901914_0.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 분류 예제(Gradio + gemma2 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow MobileNetV2 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 로컬 서버 주소\n",
    "    # 사용하려는 Ollama 모델 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama를 사용해 음식 설명 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 예측 함수\n",
    "        # URL에서 이미지 가져오기\n",
    "            # BytesIO 사용하여 이미지 열기\n",
    "        # 이미지를 배열로 변환\n",
    "            # 배치 차원 추가\n",
    "            # 전처리\n",
    "        # 예측 수행\n",
    "            # 상위 3개 예측 결과 반환\n",
    "        # 예측 결과 형식화\n",
    "        # 가장 높은 확률의 예측값으로 Ollama 설명 생성\n",
    "            # 가장 확률이 높은 음식 이름\n",
    "            # 예측 결과와 Ollama 설명 반환\n",
    "            # 에러 발생 시 기본값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "        # 상위 3개 예측 결과\n",
    "        # Ollama로 생성한 설명 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터페이스 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
